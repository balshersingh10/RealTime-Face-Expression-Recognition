{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from keras.utils import np_utils\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  35887 non-null  int64 \n",
      " 1   pixels   35887 non-null  object\n",
      " 2   Usage    35887 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data_csv_file_name = 'data/fer2013.csv'\n",
    "raw_data = pd.read_csv(raw_data_csv_file_name)\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training       28709\n",
       "PublicTest      3589\n",
       "PrivateTest     3589\n",
       "Name: Usage, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"Usage\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing images in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_image_and_label(x, y):\n",
    "    x_reshaped = x.reshape(48,48)\n",
    "    plt.imshow(x_reshaped, cmap= \"gray\",\n",
    "              interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(y)\n",
    "\n",
    "# x_pixels\n",
    "img = raw_data[\"pixels\"][0]\n",
    "val = img.split(\" \")\n",
    "x_pixels = np.array(val, 'float32')\n",
    "x_pixels /= 255\n",
    "np.shape(x_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "show_image_and_label(x_pixels, raw_data[\"emotion\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Functions For Preprocessing of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_END = 28708\n",
    "TEST_START = TRAIN_END + 1\n",
    "NUM_CLASSES = 7\n",
    "IMG_SIZE = 48\n",
    "\n",
    "def split_for_test(list):\n",
    "    train = list[0:TRAIN_END]\n",
    "    test = list[TEST_START:]\n",
    "    return train, test\n",
    "\n",
    "def pandas_vector_to_list(pandas_df):\n",
    "    py_list = [item[0] for item in pandas_df.values.tolist()]\n",
    "    return py_list\n",
    "\n",
    "def process_emotion(emotion):\n",
    "    \"\"\"\n",
    "    Takes in a vector of emotions and outputs a list of emotions as one-hot vectors.\n",
    "    :param emotion: vector of ints (0-7)\n",
    "    :return: list of one-hot vectors (array of 7)\n",
    "    \"\"\"\n",
    "    emotion_as_list = pandas_vector_to_list(emotion)\n",
    "    y_data = []\n",
    "    for index in range(len(emotion_as_list)):\n",
    "        y_data.append(emotion_as_list[index])\n",
    "\n",
    "    # Y data\n",
    "    y_data_categorical = np_utils.to_categorical(y_data, NUM_CLASSES)\n",
    "    return y_data_categorical\n",
    "\n",
    "def process_pixels(pixels, img_size=48):\n",
    "    \"\"\"\n",
    "    Takes in a string (pixels) that has space separated ints. Will transform the ints\n",
    "    to a 48x48 matrix of floats(/255).\n",
    "    :param pixels: string with space separated ints\n",
    "    :param img_size: image size\n",
    "    :return: array of 48x48 matrices\n",
    "    \"\"\"\n",
    "    pixels_as_list = pandas_vector_to_list(pixels)\n",
    "\n",
    "    np_image_array = []\n",
    "    for index, item in enumerate(pixels_as_list):\n",
    "        # 48x48\n",
    "        data = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "        # split space separated ints\n",
    "        pixel_data = item.split()\n",
    "\n",
    "        # 0 -> 47, loop through the rows\n",
    "        for i in range(0, img_size):\n",
    "            # (0 = 0), (1 = 47), (2 = 94), ...\n",
    "            pixel_index = i * img_size\n",
    "            # (0 = [0:47]), (1 = [47: 94]), (2 = [94, 141]), ...\n",
    "            data[i] = pixel_data[pixel_index:pixel_index + img_size]\n",
    "\n",
    "        np_image_array.append(np.array(data))\n",
    "\n",
    "    np_image_array = np.array(np_image_array)\n",
    "    # convert to float and divide by 255\n",
    "    np_image_array = np_image_array.astype('float32') / 255.0\n",
    "    return np_image_array\n",
    "\n",
    "def duplicate_input_layer(array_input, size):\n",
    "    vg_input = np.empty([size, 48, 48, 3])\n",
    "    for index, item in enumerate(vg_input):\n",
    "        item[:, :, 0] = array_input[index]\n",
    "        item[:, :, 1] = array_input[index]\n",
    "        item[:, :, 2] = array_input[index]\n",
    "    return vg_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one hot vectors\n",
    "emotion_array = process_emotion(raw_data[['emotion']])\n",
    "# convert to a 48x48 float matrix\n",
    "pixel_array = process_pixels(raw_data[['pixels']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for test/train\n",
    "y_train, y_test = split_for_test(emotion_array)\n",
    "x_train_matrix, x_test_matrix = split_for_test(pixel_array)\n",
    "n_train = int(len(x_train_matrix))\n",
    "n_test = int(len(x_test_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_input = duplicate_input_layer(x_train_matrix, n_train)\n",
    "x_test_input = duplicate_input_layer(x_test_matrix, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 16s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     [(None, 48, 48, 3)]       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, None, None, 512)   14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 7)                 28679     \n",
      "=================================================================\n",
      "Total params: 33,625,927\n",
      "Trainable params: 33,625,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False,)\n",
    "model_vgg16_conv.summary()\n",
    "\n",
    "inp = Input(shape=(48, 48, 3),name = 'image_input')\n",
    "\n",
    "#Use the generated model \n",
    "output_vgg16_conv = model_vgg16_conv(inp)\n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = Dense(7, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#Create your own model \n",
    "my_model = Model(inp,x)\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28708, 48, 48, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer for Model and Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "my_model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Checkpoint Callback for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"current_training/checkpoint/cp-{epoch:04d}.ckpt\"\n",
    "#checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(x_train_input, y_train,\n",
    "          epochs=80,\n",
    "          batch_size=32,\n",
    "          validation_data=(x_test_input, y_test),\n",
    "          callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "my_model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Model with Weights included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 48, 48, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 14,945,607\n",
      "Trainable params: 14,945,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()\n",
    "\n",
    "# Restore the weights\n",
    "#new_model.load_weights('cp-0074.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Accuracy on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 - 197s - loss: 0.0041 - accuracy: 0.9979\n",
      "Restored model, accuracy: 99.79%\n",
      "(28708, 7)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the restored model\n",
    "loss, acc = new_model.evaluate(x_train_input,  y_train, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
    "\n",
    "print(new_model.predict(x_train_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 42s - loss: 3.0060 - accuracy: 0.6775\n",
      "Restored model, accuracy: 67.75%\n",
      "(7178, 7)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the restored model\n",
    "loss, acc = new_model.evaluate(x_test_input,  y_test, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
    "\n",
    "print(new_model.predict(x_test_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict from a single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMOTION_DICT = {1:\"ANGRY\", 2:\"DISGUST\", 3:\"FEAR\", 4:\"HAPPY\", 5:\"SAD\", 6:\"SURPRISE\", 7:\"NEUTRAL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "path = 'fear2.jpg'\n",
    "img = cv2.imread(path)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite(path, gray)\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "img = cv2.imread(path)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    face_clip = img[y:y+h, x:x+w]\n",
    "    cv2.imwrite(path, cv2.resize(face_clip, (48, 48)))\n",
    "read_image = cv2.imread(path)\n",
    "read_image = read_image.reshape(1, read_image.shape[0], read_image.shape[1], read_image.shape[2])\n",
    "read_image_final = read_image/255.0\n",
    "top_pred = new_model.predict(read_image_final)\n",
    "emotion_label = top_pred[0].argmax() + 1\n",
    "emotion_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
